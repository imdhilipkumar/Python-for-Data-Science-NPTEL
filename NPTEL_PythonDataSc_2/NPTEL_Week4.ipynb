{"cells":[{"cell_type":"markdown","id":"c83fd7cd","metadata":{"id":"c83fd7cd"},"source":["# Data Science - Case Study"]},{"cell_type":"markdown","id":"3a71bb3c","metadata":{"id":"3a71bb3c"},"source":["Classification is a supervised machine learning task that involves assigning predefined labels or categories to input data based on its features. The goal is to build a model that learns the patterns in the data and can accurately predict the label or category of new, unseen data. It's commonly used for tasks like spam email detection, disease diagnosis, sentiment analysis, image recognition, and much more.\n","\n","Let's go through a simple case study to understand classification better:\n","\n","Case Study: Email Spam Detection\n","\n","Problem Statement: You work for an email service provider and want to develop an automated system that can classify incoming emails as either spam or not spam (ham).\n","\n","Data: You have a dataset of emails, each labeled as spam or ham, along with the text content of the emails.\n","\n","Steps Involved:\n","\n","Data Collection and Preprocessing:\n","\n","Gather a labeled dataset of emails, where each email is labeled as spam or ham.\n","Preprocess the text data by removing special characters, converting to lowercase, and tokenizing the text into words.\n","Feature Extraction:\n","\n","Convert the text data into numerical features that machine learning algorithms can work with. This could involve techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings.\n","Model Selection:\n","\n","Choose a classification algorithm. Common choices include:\n","Logistic Regression\n","Naive Bayes\n","Support Vector Machines\n","Decision Trees\n","Random Forests\n","Neural Networks\n","Model Training:\n","\n","Split the dataset into training and testing sets. The training set is used to train the model, and the testing set is used to evaluate its performance.\n","Train the chosen classification model using the training data.\n","Model Evaluation:\n","\n","Evaluate the model's performance using metrics such as accuracy, precision, recall, F1-score, and confusion matrix on the testing data.\n","Adjust model parameters and features to improve performance if necessary.\n","Model Deployment and Prediction:\n","\n","Once satisfied with the model's performance, deploy it to make predictions on new, unseen emails.\n","When a new email comes in, preprocess its text, extract features, and use the trained model to predict whether it's spam or ham.\n","Ongoing Monitoring and Maintenance:\n","\n","Continuously monitor the model's performance on real-world data.\n","Re-evaluate and retrain the model periodically to account for changing patterns in spam emails.\n","This case study illustrates how classification is applied to a real-world problem. The main steps involve data preprocessing, feature extraction, model selection, training, evaluation, deployment, and ongoing maintenance.\n","\n","Remember that different classification algorithms might perform differently on different datasets, and the choice of algorithm depends on the nature of the data and the specific problem you're trying to solve."]},{"cell_type":"code","execution_count":1,"id":"2d13277f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2d13277f","executionInfo":{"status":"ok","timestamp":1739621840465,"user_tz":-330,"elapsed":4461,"user":{"displayName":"Nilakshi Senapati","userId":"09589971479936061723"}},"outputId":"5d1ecb0b-b392-4333-d10e-0446edf66054"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.00%\n"]}],"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Sample dataset: weight in grams and color (0 for red, 1 for orange)\n","data = np.array([[100, 0], [130, 0], [135, 1], [150, 1]])\n","labels = np.array([0, 0, 1, 1])  # 0: Apple, 1: Orange\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n","\n","# Create a logistic regression model\n","model = LogisticRegression()\n","\n","# Train the model on the training data\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the testing data\n","predictions = model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, predictions)\n","\n","# Print the accuracy\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")"]},{"cell_type":"code","source":["print('data :\\n',data)\n","print('X_train : \\n',X_train)\n","print('y_train :\\n',y_train)\n","print('X_test : \\n',X_test)\n","print('y_test : \\n',y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hxx-nniNZbhz","executionInfo":{"status":"ok","timestamp":1739621905102,"user_tz":-330,"elapsed":381,"user":{"displayName":"Nilakshi Senapati","userId":"09589971479936061723"}},"outputId":"b3fcf7e9-eab7-455e-9bed-4d9ef23b4c6f"},"id":"Hxx-nniNZbhz","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["data :\n"," [[100   0]\n"," [130   0]\n"," [135   1]\n"," [150   1]]\n","X_train : \n"," [[150   1]\n"," [100   0]\n"," [135   1]]\n","y_train :\n"," [1 0 1]\n","X_test : \n"," [[130   0]]\n","y_test : \n"," [0]\n"]}]},{"cell_type":"code","source":["train_test_split?"],"metadata":{"id":"tmREPhNSY8af","executionInfo":{"status":"ok","timestamp":1739621990448,"user_tz":-330,"elapsed":424,"user":{"displayName":"Nilakshi Senapati","userId":"09589971479936061723"}}},"id":"tmREPhNSY8af","execution_count":3,"outputs":[]},{"cell_type":"markdown","id":"e40267d8","metadata":{"id":"e40267d8"},"source":["In this example :\n","1. We have a simple dataset with weight and color as features and corresponding labels (0 for Apple, 1 for Orange).\n","2. We split the dataset into training and testing sets using train_test_split.\n","3. We create a LogisticRegression model and train it using the training data.\n","4. We make predictions on the testing data and calculate the accuracy using accuracy_score."]},{"cell_type":"code","execution_count":4,"id":"2617e441","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2617e441","executionInfo":{"status":"ok","timestamp":1739622337217,"user_tz":-330,"elapsed":765,"user":{"displayName":"Nilakshi Senapati","userId":"09589971479936061723"}},"outputId":"ef58780b-7ff0-47ae-f2be-1de6c4f85cc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 100.00%\n"]}],"source":["import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data  # Features\n","y = iris.target  # Labels\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create a Random Forest classifier\n","model = RandomForestClassifier()\n","\n","# Train the model on the training data\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the testing data\n","predictions = model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, predictions)\n","\n","# Print the accuracy\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")"]},{"cell_type":"code","source":["X"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46sETwfclQkC","executionInfo":{"status":"ok","timestamp":1739622524584,"user_tz":-330,"elapsed":3,"user":{"displayName":"Nilakshi Senapati","userId":"09589971479936061723"}},"outputId":"e4d76078-550b-4fa4-8aad-754b83e32ce1"},"id":"46sETwfclQkC","execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[5.1, 3.5, 1.4, 0.2],\n","       [4.9, 3. , 1.4, 0.2],\n","       [4.7, 3.2, 1.3, 0.2],\n","       [4.6, 3.1, 1.5, 0.2],\n","       [5. , 3.6, 1.4, 0.2],\n","       [5.4, 3.9, 1.7, 0.4],\n","       [4.6, 3.4, 1.4, 0.3],\n","       [5. , 3.4, 1.5, 0.2],\n","       [4.4, 2.9, 1.4, 0.2],\n","       [4.9, 3.1, 1.5, 0.1],\n","       [5.4, 3.7, 1.5, 0.2],\n","       [4.8, 3.4, 1.6, 0.2],\n","       [4.8, 3. , 1.4, 0.1],\n","       [4.3, 3. , 1.1, 0.1],\n","       [5.8, 4. , 1.2, 0.2],\n","       [5.7, 4.4, 1.5, 0.4],\n","       [5.4, 3.9, 1.3, 0.4],\n","       [5.1, 3.5, 1.4, 0.3],\n","       [5.7, 3.8, 1.7, 0.3],\n","       [5.1, 3.8, 1.5, 0.3],\n","       [5.4, 3.4, 1.7, 0.2],\n","       [5.1, 3.7, 1.5, 0.4],\n","       [4.6, 3.6, 1. , 0.2],\n","       [5.1, 3.3, 1.7, 0.5],\n","       [4.8, 3.4, 1.9, 0.2],\n","       [5. , 3. , 1.6, 0.2],\n","       [5. , 3.4, 1.6, 0.4],\n","       [5.2, 3.5, 1.5, 0.2],\n","       [5.2, 3.4, 1.4, 0.2],\n","       [4.7, 3.2, 1.6, 0.2],\n","       [4.8, 3.1, 1.6, 0.2],\n","       [5.4, 3.4, 1.5, 0.4],\n","       [5.2, 4.1, 1.5, 0.1],\n","       [5.5, 4.2, 1.4, 0.2],\n","       [4.9, 3.1, 1.5, 0.2],\n","       [5. , 3.2, 1.2, 0.2],\n","       [5.5, 3.5, 1.3, 0.2],\n","       [4.9, 3.6, 1.4, 0.1],\n","       [4.4, 3. , 1.3, 0.2],\n","       [5.1, 3.4, 1.5, 0.2],\n","       [5. , 3.5, 1.3, 0.3],\n","       [4.5, 2.3, 1.3, 0.3],\n","       [4.4, 3.2, 1.3, 0.2],\n","       [5. , 3.5, 1.6, 0.6],\n","       [5.1, 3.8, 1.9, 0.4],\n","       [4.8, 3. , 1.4, 0.3],\n","       [5.1, 3.8, 1.6, 0.2],\n","       [4.6, 3.2, 1.4, 0.2],\n","       [5.3, 3.7, 1.5, 0.2],\n","       [5. , 3.3, 1.4, 0.2],\n","       [7. , 3.2, 4.7, 1.4],\n","       [6.4, 3.2, 4.5, 1.5],\n","       [6.9, 3.1, 4.9, 1.5],\n","       [5.5, 2.3, 4. , 1.3],\n","       [6.5, 2.8, 4.6, 1.5],\n","       [5.7, 2.8, 4.5, 1.3],\n","       [6.3, 3.3, 4.7, 1.6],\n","       [4.9, 2.4, 3.3, 1. ],\n","       [6.6, 2.9, 4.6, 1.3],\n","       [5.2, 2.7, 3.9, 1.4],\n","       [5. , 2. , 3.5, 1. ],\n","       [5.9, 3. , 4.2, 1.5],\n","       [6. , 2.2, 4. , 1. ],\n","       [6.1, 2.9, 4.7, 1.4],\n","       [5.6, 2.9, 3.6, 1.3],\n","       [6.7, 3.1, 4.4, 1.4],\n","       [5.6, 3. , 4.5, 1.5],\n","       [5.8, 2.7, 4.1, 1. ],\n","       [6.2, 2.2, 4.5, 1.5],\n","       [5.6, 2.5, 3.9, 1.1],\n","       [5.9, 3.2, 4.8, 1.8],\n","       [6.1, 2.8, 4. , 1.3],\n","       [6.3, 2.5, 4.9, 1.5],\n","       [6.1, 2.8, 4.7, 1.2],\n","       [6.4, 2.9, 4.3, 1.3],\n","       [6.6, 3. , 4.4, 1.4],\n","       [6.8, 2.8, 4.8, 1.4],\n","       [6.7, 3. , 5. , 1.7],\n","       [6. , 2.9, 4.5, 1.5],\n","       [5.7, 2.6, 3.5, 1. ],\n","       [5.5, 2.4, 3.8, 1.1],\n","       [5.5, 2.4, 3.7, 1. ],\n","       [5.8, 2.7, 3.9, 1.2],\n","       [6. , 2.7, 5.1, 1.6],\n","       [5.4, 3. , 4.5, 1.5],\n","       [6. , 3.4, 4.5, 1.6],\n","       [6.7, 3.1, 4.7, 1.5],\n","       [6.3, 2.3, 4.4, 1.3],\n","       [5.6, 3. , 4.1, 1.3],\n","       [5.5, 2.5, 4. , 1.3],\n","       [5.5, 2.6, 4.4, 1.2],\n","       [6.1, 3. , 4.6, 1.4],\n","       [5.8, 2.6, 4. , 1.2],\n","       [5. , 2.3, 3.3, 1. ],\n","       [5.6, 2.7, 4.2, 1.3],\n","       [5.7, 3. , 4.2, 1.2],\n","       [5.7, 2.9, 4.2, 1.3],\n","       [6.2, 2.9, 4.3, 1.3],\n","       [5.1, 2.5, 3. , 1.1],\n","       [5.7, 2.8, 4.1, 1.3],\n","       [6.3, 3.3, 6. , 2.5],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [7.1, 3. , 5.9, 2.1],\n","       [6.3, 2.9, 5.6, 1.8],\n","       [6.5, 3. , 5.8, 2.2],\n","       [7.6, 3. , 6.6, 2.1],\n","       [4.9, 2.5, 4.5, 1.7],\n","       [7.3, 2.9, 6.3, 1.8],\n","       [6.7, 2.5, 5.8, 1.8],\n","       [7.2, 3.6, 6.1, 2.5],\n","       [6.5, 3.2, 5.1, 2. ],\n","       [6.4, 2.7, 5.3, 1.9],\n","       [6.8, 3. , 5.5, 2.1],\n","       [5.7, 2.5, 5. , 2. ],\n","       [5.8, 2.8, 5.1, 2.4],\n","       [6.4, 3.2, 5.3, 2.3],\n","       [6.5, 3. , 5.5, 1.8],\n","       [7.7, 3.8, 6.7, 2.2],\n","       [7.7, 2.6, 6.9, 2.3],\n","       [6. , 2.2, 5. , 1.5],\n","       [6.9, 3.2, 5.7, 2.3],\n","       [5.6, 2.8, 4.9, 2. ],\n","       [7.7, 2.8, 6.7, 2. ],\n","       [6.3, 2.7, 4.9, 1.8],\n","       [6.7, 3.3, 5.7, 2.1],\n","       [7.2, 3.2, 6. , 1.8],\n","       [6.2, 2.8, 4.8, 1.8],\n","       [6.1, 3. , 4.9, 1.8],\n","       [6.4, 2.8, 5.6, 2.1],\n","       [7.2, 3. , 5.8, 1.6],\n","       [7.4, 2.8, 6.1, 1.9],\n","       [7.9, 3.8, 6.4, 2. ],\n","       [6.4, 2.8, 5.6, 2.2],\n","       [6.3, 2.8, 5.1, 1.5],\n","       [6.1, 2.6, 5.6, 1.4],\n","       [7.7, 3. , 6.1, 2.3],\n","       [6.3, 3.4, 5.6, 2.4],\n","       [6.4, 3.1, 5.5, 1.8],\n","       [6. , 3. , 4.8, 1.8],\n","       [6.9, 3.1, 5.4, 2.1],\n","       [6.7, 3.1, 5.6, 2.4],\n","       [6.9, 3.1, 5.1, 2.3],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [6.8, 3.2, 5.9, 2.3],\n","       [6.7, 3.3, 5.7, 2.5],\n","       [6.7, 3. , 5.2, 2.3],\n","       [6.3, 2.5, 5. , 1.9],\n","       [6.5, 3. , 5.2, 2. ],\n","       [6.2, 3.4, 5.4, 2.3],\n","       [5.9, 3. , 5.1, 1.8]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","id":"8eb0d632","metadata":{"id":"8eb0d632"},"source":["1. We're using the Iris dataset, which has more complex features.\n","2. We're using a RandomForestClassifier, which is an ensemble method that can capture complex relationships in the data.\n","3. We split the dataset into training and testing sets using train_test_split.\n","4. We train the model on the training data and predict the labels for the testing data.\n","5. We calculate the accuracy using accuracy_score."]},{"cell_type":"markdown","id":"2f39c5c4","metadata":{"id":"2f39c5c4"},"source":["import numpy as np: This imports the NumPy library and aliases it as np.\n","\n","from sklearn.datasets import load_iris: This imports the load_iris function from the scikit-learn library's datasets module. The load_iris function provides the famous Iris dataset for classification.\n","\n","from sklearn.model_selection import train_test_split: This imports the train_test_split function, which is used to split the dataset into training and testing sets.\n","\n","from sklearn.ensemble import RandomForestClassifier: This imports the RandomForestClassifier class, which is an ensemble learning method used for classification.\n","\n","from sklearn.metrics import accuracy_score: This imports the accuracy_score function, which calculates the accuracy of a classification model.\n","\n","iris = load_iris(): This loads the Iris dataset.\n","X = iris.data: This assigns the features (input variables) of the Iris dataset to the variable X.\n","y = iris.target: This assigns the target labels (output variable) of the Iris dataset to the variable y.\n","\n","train_test_split(X, y, test_size=0.2, random_state=42): This function splits the data into training and testing sets. X_train and y_train will contain the training data and labels, while X_test and y_test will contain the testing data and labels. test_size=0.2 specifies that 20% of the data will be used for testing, and random_state=42 sets a seed for reproducibility.\n","\n","model = RandomForestClassifier(): This creates an instance of the RandomForestClassifier class, which will be used as our classification model.\n","\n","model.fit(X_train, y_train): This trains the model using the training data (X_train and y_train).\n","\n","predictions = model.predict(X_test): This uses the trained model to make predictions on the testing data (X_test).\n","\n","accuracy = accuracy_score(y_test, predictions): This calculates the accuracy of the model's predictions by comparing the predicted labels (predictions) with the actual labels (y_test).\n","\n","print(f\"Accuracy: {accuracy * 100:.2f}%\"): This prints the accuracy as a percentage with two decimal places."]},{"cell_type":"code","execution_count":6,"id":"e6091328","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e6091328","executionInfo":{"status":"ok","timestamp":1739622840010,"user_tz":-330,"elapsed":413,"user":{"displayName":"Nilakshi Senapati","userId":"09589971479936061723"}},"outputId":"66aefecb-e336-4df4-da53-1472fcde6c5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error: 810000.00\n","R-squared: nan\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n","  warnings.warn(msg, UndefinedMetricWarning)\n"]}],"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# Sample dataset: mileage in miles, age in years, brand (0 for Toyota, 1 for Honda)\n","data = np.array([[50000, 3, 0], [80000, 5, 1], [20000, 1, 0], [60000, 4, 1]])\n","prices = np.array([15000, 12000, 18000, 13000])  # Prices in dollars\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(data, prices, test_size=0.2, random_state=42)\n","\n","# Create a Linear Regression model\n","model = LinearRegression()\n","\n","# Train the model on the training data\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the testing data\n","predictions = model.predict(X_test)\n","\n","# Calculate Mean Squared Error and R-squared\n","mse = mean_squared_error(y_test, predictions)\n","r2 = r2_score(y_test, predictions)\n","\n","# Print the results\n","print(f\"Mean Squared Error: {mse:.2f}\")\n","print(f\"R-squared: {r2:.2f}\")\n"]},{"cell_type":"markdown","id":"da707c1b","metadata":{"id":"da707c1b"},"source":["1. We have a simple dataset with mileage, age, and brand as features, and the corresponding prices of used cars.\n","2. We split the dataset into training and testing sets using train_test_split.\n","3. We create a LinearRegression model and train it using the training data.\n","4. We make predictions on the testing data and calculate the Mean Squared Error (MSE) and R-squared using mean_squared_error and r2_score."]},{"cell_type":"code","execution_count":8,"id":"8b229e11","metadata":{"id":"8b229e11","outputId":"a48eecb6-cce2-4141-c5fc-3337898a72a1","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1739623082506,"user_tz":-330,"elapsed":426,"user":{"displayName":"Nilakshi Senapati","userId":"09589971479936061723"}}},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-6f12fa136dd3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \"\"\"\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import numpy as np\n","from sklearn.datasets import load_boston\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# Load the Boston Housing dataset\n","boston = load_boston()\n","X = boston.data  # Features\n","y = boston.target  # Prices\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create a Linear Regression model\n","model = LinearRegression()\n","\n","# Train the model on the training data\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the testing data\n","predictions = model.predict(X_test)\n","\n","# Calculate Mean Squared Error and R-squared\n","mse = mean_squared_error(y_test, predictions)\n","r2 = r2_score(y_test, predictions)\n","\n","# Print the results\n","print(f\"Mean Squared Error: {mse:.2f}\")\n","print(f\"R-squared: {r2:.2f}\")"]},{"cell_type":"markdown","id":"7b083e91","metadata":{"id":"7b083e91"},"source":["Linear Regression is a fundamental supervised learning algorithm used for predicting a continuous target variable based on one or more input features. It assumes a linear relationship between the input features and the target variable.\n","\n","In the context of a simple linear regression with one feature (univariate linear regression), the model can be represented as:\n","y = mx + b\n","\n","Where:\n","\n","y is the predicted target variable (output).\n","x is the input feature.\n","m is the slope (weight) of the line.\n","b is the y-intercept.\n","\n","The goal of linear regression is to find the values of m and b that minimize the difference between the predicted values and the actual target values. This is typically achieved using a mathematical optimization technique called the Ordinary Least Squares (OLS) method."]},{"cell_type":"markdown","id":"22979bd5","metadata":{"id":"22979bd5"},"source":["In the context of multiple features (multivariate linear regression), the model equation becomes:\n","y = b0 + b1*x1 + b2*x2 + ... + bn*xn\n","\n","Where:\n","\n","y is the predicted target variable.\n","x1, x2, ..., xn are the input features.\n","b0, b1, ..., bn are the coefficients (weights) associated with the features."]},{"cell_type":"markdown","id":"05442bf2","metadata":{"id":"05442bf2"},"source":["Here are the steps involved in implementing Linear Regression:\n","\n","1. Data Preparation: Collect and preprocess the dataset. This includes handling missing values, encoding categorical variables, and splitting the data into training and testing sets.\n","\n","2. Model Creation: Choose the type of linear regression (simple or multiple) and create the linear regression model object using a suitable library (e.g., scikit-learn in Python).\n","\n","3. Model Training: Fit the model to the training data, which involves estimating the coefficients that minimize the difference between predicted and actual values.\n","\n","4. Prediction: Use the trained model to make predictions on new, unseen data.\n","\n","5. Evaluation: Evaluate the model's performance using appropriate metrics such as Mean Squared Error (MSE), R-squared (coefficient of determination), and others.\n","\n","6. Interpretation: Interpret the coefficients to understand the relationship between the features and the target variable. Positive coefficients indicate a positive correlation, while negative coefficients indicate a negative correlation.\n","\n","7. Prediction and Generalization: Use the trained model to predict outcomes on new data. Ensure that the model generalizes well to unseen data."]},{"cell_type":"markdown","id":"7f27f29a","metadata":{"id":"7f27f29a"},"source":["Assumptions of Linear Regression:\n","\n","1. Linearity: The relationship between the input features and the target variable should be approximately linear.\n","2. Independence: The errors (residuals) should be independent of each other.\n","3. Homoscedasticity: The variance of the errors should be roughly constant across all levels of the target variable.\n","4. Normality: The errors should be normally distributed.\n","\n","Remember that while Linear Regression is a powerful and widely used algorithm, it might not be suitable for all types of data and relationships. In some cases, more complex algorithms like Polynomial Regression, Decision Trees, or Neural Networks might be needed to capture nonlinear relationships in the data."]},{"cell_type":"code","execution_count":null,"id":"fcf3ac7b","metadata":{"id":"fcf3ac7b"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}